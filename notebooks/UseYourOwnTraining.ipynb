{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/walkerlab/FENS-2022/blob/main/notebooks/UseYourOwnTraining.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/walkerlab/FENS-2022/blob/main/notebooks/UseYourOwnTraining.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp0SO4gr2D7w"
   },
   "source": [
    "# Digging deeper into the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [main notebook](./Deep-Learning-in-Neuroscience.ipynb), we made exclusive use of `train_model` function to take care of a lot of details of the training. In this notebook, we are going to develop our own, albeit simpler training routine to get a better appreciation of what goes into training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are going to prepare the environment by downloading the necessary library (e.g. `FENS-2022`) and the dataset. This is necessary as each colab notebook ends up offering distinct environment by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install the FENS package\n",
    "!git clone https://github.com/walkerlab/FENS-2022.git\n",
    "!pip3 install FENS-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsNehEZlTSL4",
    "outputId": "8e2984ad-9f32-4dfd-b8b7-4536f2e53823"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "!wget -nc \"https://onedrive.live.com/download?cid=06D44059794C5B46&resid=6D44059794C5B46%21121992&authkey=AHJVfxtvAASasjQ\" -O dataset.zip\n",
    "\n",
    "# Unzip\n",
    "!unzip -nq 'dataset.zip'\n",
    "\n",
    "# get trained network weights\n",
    "!git clone https://gin.g-node.org/walkerlab/fens-2022.git /content/trained_nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we go ahead and implment a bunch of standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we prepare PyTorch [dataloaders](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to conveniently load all images and responses in **batches** using `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fens.dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = load_dataset(path = './Lurz2020/static20457-5-9-preproc0', batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaders['train']\n",
    "valid_loader = dataloaders['validation']\n",
    "test_loader = dataloaders['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also extract an underlying dataset object so that we can gain access to additional *meta* information. Keep in mind that additional data attributes avaialble on the dataset is unique to the way we designed the dataset in `fens` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to the dataset object that underlies all dataloaders\n",
    "dataset = dataloaders['test'].dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that indeed a single neuron's responses vary widely even to repeated presentations of an identical stimulus!\n",
    "\n",
    "This so-called **noiseness** of neural responses make predicting the respones of the neurons to images fundamentally challenging and in fact makes it  essentially impossible to yield a perfect fit!\n",
    "\n",
    "Instead, we would often try to fit the **distribution of responses** the best we can, and we will briefly visit this point later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the LN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again define a Linear-Nonlinear (LN) model that we can use to test our training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        momentum=0.1,\n",
    "        init_std=1e-3,\n",
    "        gamma=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(1, momentum=momentum, affine=False)\n",
    "        self.linear = nn.Linear(input_height * input_width, n_neurons)\n",
    "        self.gamma = gamma\n",
    "        self.init_std = init_std\n",
    "        self.initialize()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.linear(x.flatten(1))\n",
    "        return nn.functional.elu(x) + 1\n",
    "        \n",
    "\n",
    "    def initialize(self, std=None):\n",
    "        if std is None:\n",
    "            std = self.init_std\n",
    "        nn.init.normal_(self.linear.weight.data, std=std)\n",
    "\n",
    "\n",
    "    def regularizer(self):\n",
    "        return self.gamma * self.linear.weight.abs().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the training - simple routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train the network, and this time we will do so wihout relying on the `fens`' `train_model` convenience function. \n",
    "\n",
    "We are going to want to periodically check the performance on the validation set during the training to monitor the progress. It's fairly common to use a metric here that is different from the training objective and more intuitive. \n",
    "\n",
    "Here, we are going to compute `correlation` between the real and the predicted responses across images, averaged over the neurons. Let's go ahead and define a function to compute the correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y1, y2, axis=-1, eps=1e-8, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the correlation between two matrices along certain dimensions.\n",
    "\n",
    "    Args:\n",
    "        y1:      first numpy array\n",
    "        y2:      second numpy array\n",
    "        axis:    dimension along which the correlation is computed.\n",
    "        eps:     offset to the standard deviation to make sure the correlation is well defined (default 1e-8)\n",
    "        **kwargs passed to final mean of standardized y1 * y2\n",
    "\n",
    "    Returns: correlation vector\n",
    "\n",
    "    \"\"\"\n",
    "    y1 = (y1 - y1.mean(axis=axis, keepdims=True)) / (\n",
    "        y1.std(axis=axis, keepdims=True, ddof=0) + eps\n",
    "    )\n",
    "    y2 = (y2 - y2.mean(axis=axis, keepdims=True)) / (\n",
    "        y2.std(axis=axis, keepdims=True, ddof=0) + eps\n",
    "    )\n",
    "    return (y1 * y2).mean(axis=axis, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set various part go ahead and train the network. Let's go ahead and build our network and define our training routine, using a simple `SGD` (stochastic gradient descient optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first instantiate the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_model = Linear(input_height=64, input_width=36, n_neurons=5335, gamma=0.1)\n",
    "\n",
    "# let us refer to our target model as `model`\n",
    "model = ln_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated a network to train, let's define a routine to train the network. We will train on our training dataset, and periodically report the performance on the validation dataset. As discussed before, we will make use of Poisson Loss, which is simply the negative log likelihood of Poisson probability. This is conveniently given as `PoissonNLLLoss` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import PoissonNLLLoss\n",
    "from scipy import percentile\n",
    "from tqdm import tqdm # to show progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "REPORT_FREQ = 2 # how often to report the performance on the validation set\n",
    "lr = 0.01\n",
    "device = 'cuda'\n",
    "\n",
    "# log_input=False says that we intend to pass in response value directly, rather than log of responses\n",
    "criterion = PoissonNLLLoss(log_input=False, reduction='sum')\n",
    "#criterion = PoissonLoss(avg=False)\n",
    "\n",
    "# move model and criterion into the target device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# define and setup the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "valid_loader = dataloaders['validation']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for batch_no, (images, targets) in tqdm(enumerate(train_loader), \n",
    "                                            desc=\"Epoch {}\".format(epoch),\n",
    "                                            total=len(train_loader)):\n",
    "        # put model into training mode\n",
    "        model.train()\n",
    "\n",
    "        # zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move data into the target device\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # get predicted responses\n",
    "        responses = model(images)\n",
    "\n",
    "        # compute the loss, with regularizers\n",
    "        loss = criterion(responses, targets) + model.regularizer()\n",
    "        \n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # apply the learning step\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % REPORT_FREQ == 0:\n",
    "        total_responses = []\n",
    "        total_targets = []\n",
    "        for images, targets in tqdm(valid_loader,\n",
    "                                    desc=\"Validation\",\n",
    "                                    total=len(valid_loader)):\n",
    "            pass\n",
    "            with torch.no_grad():\n",
    "                images = images.to(device)\n",
    "                model.eval()\n",
    "                total_responses.append(model(images).detach().cpu())\n",
    "            total_targets.append(targets.detach().cpu())\n",
    "\n",
    "        # concatenate batches into one big numpy array\n",
    "        total_responses = torch.concat(total_responses).numpy()\n",
    "        total_targets = torch.concat(total_targets).numpy()\n",
    "        \n",
    "        # compute the correlation\n",
    "        print('Correlation: {:.3f}'.format(corr(total_responses, total_targets, axis=0).mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try and see how well the above routine works when you use a CNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        momentum=0.1,\n",
    "        init_std=1e-3,\n",
    "        gamma=0.1,\n",
    "        hidden_channels=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_std = init_std\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # CNN core\n",
    "        self.cnn_core = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", nn.Conv2d(1, hidden_channels, 15, padding=15 // 2, bias=False)),\n",
    "                    (\"bn1\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu1\", nn.ELU()),\n",
    "                    (\"conv2\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn2\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu2\", nn.ELU()),\n",
    "                    (\"conv3\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn3\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu3\", nn.ELU()),\n",
    "                    (\"conv4\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn4\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu4\", nn.ELU()),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Fully connected readout\n",
    "        self.readout = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    ('fc_ro', nn.Linear(input_height * input_width * hidden_channels, n_neurons)),\n",
    "                    ('bn_ro', nn.BatchNorm1d(n_neurons, momentum=momentum)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def initialize(self, std=None):\n",
    "        if std is None:\n",
    "            std = self.init_std\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_core(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.readout(x)\n",
    "        return nn.functional.elu(x) + 1\n",
    "    \n",
    "    def regularizer(self):\n",
    "        return self.readout[0].weight.abs().sum() * self.gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now instantiate the model and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(input_height=64, input_width=36, n_neurons=5335)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
